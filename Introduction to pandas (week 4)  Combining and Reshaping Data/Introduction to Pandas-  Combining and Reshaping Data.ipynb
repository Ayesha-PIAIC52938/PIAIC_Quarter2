{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0383c960",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" >\n",
    "    \n",
    "# <div class=\"alert alert-success\" > Combining and Reshaping Data\n",
    "    \n",
    "Everything that we did focused upon working within the data of a single DataFrame or Series object, and keeping the same structure of data within those objects. Once the data is tidied up, it will be likely that we will then need to use this data either to combine multiple sets of data, or to reorganize the structure of the data by moving data in and out of indexes.\n",
    "\n",
    "We'll cover two general categories of topics: \n",
    "    \n",
    "- The first two sections will cover the capabilities provided by pandas to combine the data from multiple pandas objects together. Combination of data in pandas is performed by <b>concatenating</b> two sets of data, where data is combined simply along either axes but without regard to relationships in the data. Or data can be combined using relationships in the data by using a pandas capability referred to as <b>merging</b>, which provides join operations that are similar to those in many\n",
    "relational databases.\n",
    "    \n",
    "- The remaining sections will examine the three primary means reshaping data in pandas. These will examine the processes of pivoting, stacking and unstacking, and melting of data.\n",
    "\n",
    "    <b>Pivoting</b> allows us to restructure pandas data similarly to how spreadsheets pivot data by creating new index levels and moving data into columns based upon values (or vice-versa).\n",
    "    \n",
    "    <b>Stacking and unstacking</b> are similar to pivoting, but allow us to pivot data organized with multiple levels of indexes. \n",
    "    \n",
    "    <b>Melting</b> allows us to restructure data into unique ID-variable-measurement combinations that are or required for many statistical analyses.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e4eb6",
   "metadata": {},
   "source": [
    "### <div class= \"alert alert-warning\"> Setting up the IPython notebook\n",
    "    \n",
    "To utilize the examples in this chapter we will need to include the following imports\n",
    "and settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e47fe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "    \n",
    "# Set some pandas options for controlling output\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2aabe3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" >\n",
    "    \n",
    "## <div class= \"alert alert-info\">Concatenating data\n",
    "    \n",
    "Concatenation in pandas is the process of either adding rows to the end of an existing Series or DataFrame object or adding additional columns to a DataFrame. \n",
    "    \n",
    "In pandas, concatenation is performed via the pandas function <span style=\"color:red\">pd.concat()</span>. The function will perform the operation on a specific axis and as we will see, will also perform any required set logic involved in aligning along that axis.\n",
    "    \n",
    "The general syntax to concatenate data is to pass a list of objects to pd.concat(). The following performs a concatenation of two Series objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b51661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1:\n",
      " 0    0\n",
      "1    1\n",
      "2    2\n",
      "dtype: int32 \n",
      "\n",
      "s2:\n",
      " 0    5\n",
      "1    6\n",
      "2    7\n",
      "dtype: int32 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1 = pd.Series(np.arange(0, 3))\n",
    "s2 = pd.Series(np.arange(5, 8))\n",
    "print('s1:\\n',s1,'\\n')\n",
    "print('s2:\\n',s2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ad54a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    2\n",
       "0    5\n",
       "1    6\n",
       "2    7\n",
       "dtype: int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([s1, s2])         # concatenate s1 & s2 together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72689002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5\n",
       "2  6  7  8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create two DataFrame objects to concatenate. using the same index labels and column names, but different values\n",
    "df1 = pd.DataFrame(np.arange(9).reshape(3, 3),columns=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame(np.arange(9, 18).reshape(3, 3),columns=['a', 'b', 'c'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "586c4aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a   b   c\n",
       "0   9  10  11\n",
       "1  12  13  14\n",
       "2  15  16  17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a5f0f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a   b   c\n",
       "0   0   1   2\n",
       "1   3   4   5\n",
       "2   6   7   8\n",
       "0   9  10  11\n",
       "1  12  13  14\n",
       "2  15  16  17"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc = pd.concat([df1, df2])    #  concat df1 & df2\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e006d6",
   "metadata": {},
   "source": [
    "The process of concatenating the two DataFrame objects will first identify the set of columns formed by aligning the labels in the columns, effectively determining the union of the column names. The resulting DataFrame object will then consist of that\n",
    "set of columns, and columns with identical names will not be duplicated.\n",
    "\n",
    "Rows will be then be added to the result, in the order of the each of the objects passed to pd.concat(). If a column in the result does not exist in the object being copied, NaN values will be filled in those locations. Duplicate row index labels can occur.\n",
    "\n",
    "The following demonstrates the alignment of two DataFrame objects during concatenation that both have columns in common (a and c) and also have distinct columns (b in df1, and d in df2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1c2e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5\n",
       "2  6  7  8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate concatenating two DataFrame objects with different columns\n",
    "df1 = pd.DataFrame(np.arange(9).reshape(3, 3),columns=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame(np.arange(9, 18).reshape(3, 3),columns=['a', 'c', 'd'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c40a99",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd5762b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a   c   d\n",
       "0   9  10  11\n",
       "1  12  13  14\n",
       "2  15  16  17"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9ee86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a    b   c     d\n",
       "0   0  1.0   2   NaN\n",
       "1   3  4.0   5   NaN\n",
       "2   6  7.0   8   NaN\n",
       "0   9  NaN  10  11.0\n",
       "1  12  NaN  13  14.0\n",
       "2  15  NaN  16  17.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the concat, NaN values will be filled in for the d column for df1 and b column for df2\n",
    "pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f934d",
   "metadata": {},
   "source": [
    "It is possible to give each group of data in the result its own name using the <span style=\"color:green\"><b>keys</b></span> parameter. This creates a hierarchical index on the DataFrame object that lets you refer to each group of data independently via the DataFrame objects' .ix property. This is convenient if you later need to determine where data in the concatenated DataFrame object came from.\n",
    "\n",
    "The following sample demonstrates this concept by assigning names to each original DataFrame object and then retrieving the rows that originated in the df2 object, which are keyed with the label 'df2'. The following code demonstrates this labeling\n",
    "and also retrieves just the rows that originated in df2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8b920eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        a    b   c     d\n",
       "df1 0   0  1.0   2   NaN\n",
       "    1   3  4.0   5   NaN\n",
       "    2   6  7.0   8   NaN\n",
       "df2 0   9  NaN  10  11.0\n",
       "    1  12  NaN  13  14.0\n",
       "    2  15  NaN  16  17.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = pd.concat([df1, df2], keys=['df1', 'df2'])\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4ea6b",
   "metadata": {},
   "source": [
    "Accessinf df1 from C dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7632a397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a    b  c   d\n",
       "0  0  1.0  2 NaN\n",
       "1  3  4.0  5 NaN\n",
       "2  6  7.0  8 NaN"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.loc['df1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62b960e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             a    b   c     d\n",
       "DF_ID R_ID                   \n",
       "df1   0      0  1.0   2   NaN\n",
       "      1      3  4.0   5   NaN\n",
       "      2      6  7.0   8   NaN\n",
       "df2   0      9  NaN  10  11.0\n",
       "      1     12  NaN  13  14.0\n",
       "      2     15  NaN  16  17.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = pd.concat([df1, df2], keys=['df1', 'df2'], names=['DF_ID', 'R_ID'])\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904306c",
   "metadata": {},
   "source": [
    "The pd.concat() function also allows you to specify the axis on which to apply the concatenation. The following concatenates the two DataFrame objects along the columns axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf5473a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c   a   c   d\n",
       "0  0  1  2   9  10  11\n",
       "1  3  4  5  12  13  14\n",
       "2  6  7  8  15  16  17"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df1, df2], axis=1) # concat df1 and df2 along columns aligns on row labels, has duplicate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8ebeb",
   "metadata": {},
   "source": [
    "Note that the result now contains duplicate columns. The concatenation first aligns by the row index labels of each DataFrame object, and then fills in the columns from the first DataFrame object and then the second. The columns are not aligned and\n",
    "result in duplicate values. The same rules of alignment and filling of NaN values apply in this case, except that they are applied to the rows' index labels.\n",
    "\n",
    "The following demonstrates a concatenation along the columns axis with two DataFrame objects that have row index labels in\n",
    "common (2 and 3) along with disjoint rows (0 in df1 and 4 in df3). Additionally, some of the columns in df3 overlap with df1 (a) as well as being disjoint (d):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81fe706b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5\n",
       "2  6  7  8"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c40121",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3b74061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    a   d\n",
       "2  20  21\n",
       "3  22  23\n",
       "4  24  25"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame(np.arange(20, 26).reshape(3, 2),columns=['a', 'd'],index=[2, 3, 4])\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5f1ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     a    b    c     a     d\n",
       "0  0.0  1.0  2.0   NaN   NaN\n",
       "1  3.0  4.0  5.0   NaN   NaN\n",
       "2  6.0  7.0  8.0  20.0  21.0\n",
       "3  NaN  NaN  NaN  22.0  23.0\n",
       "4  NaN  NaN  NaN  24.0  25.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df1, df3], axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d63dd9",
   "metadata": {},
   "source": [
    "<mark>A concatenation of two or more DataFrame objects actually performs an outer join operation along the index labels on the axis opposite to the one specified</mark>. This makes the result of the concatenation similar to having performed a union of those index labels, and then data is filled based on the alignment of those labels to the source objects.\n",
    "\n",
    "The type of join can be changed to an inner join and can be performed by specifying<span style=\"color:green\"><b> join='inner' </b></span> as the parameter. <mark>The inner join then logically performs an intersection instead of a union</mark>. \n",
    "\n",
    "- <span style=\"color:green\"><b> join='outer': </b></span> Union of two data frames along the specified axis\n",
    "\n",
    "- <span style=\"color:green\"><b> join='inner': </b></span> intersection of two data frames along the specified axis\n",
    "\n",
    "The following demonstrates this and results in a single row because 2 is the only row index label in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d089d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c   a   d\n",
       "2  6  7  8  20  21"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df1, df3], axis=1, join='inner')   # do an inner join instead of outer results in one row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da94f0",
   "metadata": {},
   "source": [
    "It is also possible to use label groups of data along the columns using the keys parameter when applying the concatenation along axis=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41339cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  df1       df2        \n",
       "    a  b  c   a   c   d\n",
       "0   0  1  2   9  10  11\n",
       "1   3  4  5  12  13  14\n",
       "2   6  7  8  15  16  17"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.concat([df1, df2],axis=1,keys=['df1', 'df2'])    # add keys to the columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a0d7a",
   "metadata": {},
   "source": [
    "The different groups can be accessed using the <span style=\"color:red\">.get()</span> method that is used to access individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef4fac15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   a  b  c\n",
       "0  0  1  2\n",
       "1  3  4  5\n",
       "2  6  7  8"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.get('df1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2c598",
   "metadata": {},
   "source": [
    "A DataFrame (and Series) object also contains an <span style=\"color:red\">.append()</span> method, which will concatenate the two specified DataFrame objects along the row index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71275c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayesha\\AppData\\Local\\Temp\\ipykernel_9752\\3348495838.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1.append(df2) # append does a concatenate along axis=0 duplicate row index labels can result\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    a    b   c     d\n",
       "0   0  1.0   2   NaN\n",
       "1   3  4.0   5   NaN\n",
       "2   6  7.0   8   NaN\n",
       "0   9  NaN  10  11.0\n",
       "1  12  NaN  13  14.0\n",
       "2  15  NaN  16  17.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.append(df2) # append does a concatenate along axis=0 duplicate row index labels can result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf7212",
   "metadata": {},
   "source": [
    "As with a concatenation on axis=1, the index labels in the rows are copied without consideration of the creation of duplicates, and the columns labels are joined in a manner which ensures no duplicate column name is included in the result. If you would like to ensure that the resulting index does not have duplicates but preserves all of the rows, you can use the   <span style=\"color:green\"><b>ignore_index=True</b></span> parameter. This essentially returns the same result except with new Int64Index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "539b1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ayesha\\AppData\\Local\\Temp\\ipykernel_9752\\3010342201.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df1.append(df2, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "    a    b   c     d\n",
       "0   0  1.0   2   NaN\n",
       "1   3  4.0   5   NaN\n",
       "2   6  7.0   8   NaN\n",
       "3   9  NaN  10  11.0\n",
       "4  12  NaN  13  14.0\n",
       "5  15  NaN  16  17.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicates in the result index by ignoring the index labels in the source DataFrame objects\n",
    "df1.append(df2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6c116",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" >\n",
    "    \n",
    "## <div class= \"alert alert-info\">Merging and joining data\n",
    "    \n",
    "pandas allows the merging of pandas objects with database-like join operations using the <span style=\"color:red\">pd.merge()</span> function and the <span style=\"color:red\">.merge()</span> method of a DataFrame object. These joins are high performance and are performed in memory. A merge combines the data of two pandas objects by finding matching values in one or more columns\n",
    "or row indexes. It then returns a new object that represents a combination of the data from both based on relational-database-like join semantics applied to those values.\n",
    "    \n",
    "Merges are useful as they allow us to model a single DataFrame for each type of data (one of the rules of having tidy data) but to be able to relate data in different DataFrame objects using values existing in both sets of data. \n",
    "    \n",
    "## An overview of merges\n",
    "A practical and probably canonical example would be that of looking up customer names from orders. To demonstrate this in pandas, we will use the following two DataFrame objects, where one represents a list of customer details, and the other\n",
    "represents the orders made by customers and what day the order was made. They will be related to each other using the CustomerID columns in each:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5b97dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   CustomerID    Name             Address\n",
       "0          10    Mike    Address for Mike\n",
       "1          11  Marcia  Address for Marcia"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Customers\n",
    "\n",
    "customers_dic = {'CustomerID': [10, 11],\n",
    "                 'Name': ['Mike', 'Marcia'],\n",
    "                 'Address': ['Address for Mike','Address for Marcia']}\n",
    "\n",
    "customers = pd.DataFrame(customers_dic)\n",
    "customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5aba197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   CustomerID   OrderDate\n",
       "0          10  2014-12-01\n",
       "1          11  2014-12-01\n",
       "2          10  2014-12-01"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Orders made by customers. They are related to customers by CustomerID\n",
    "\n",
    "orders_dic = {'CustomerID': [10, 11, 10],\n",
    "              'OrderDate': [datetime.date(2014, 12, 1),datetime.date(2014, 12, 1),datetime.date(2014, 12, 1)]}\n",
    "\n",
    "orders = pd.DataFrame(orders_dic)\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe3994",
   "metadata": {},
   "source": [
    "Now suppose we would like to ship the orders to the customers. We would need to merge the orders data with the customers detail data to determine the address for each order. In pandas, this can be easily performed with the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "523958f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   CustomerID    Name             Address   OrderDate\n",
       "0          10    Mike    Address for Mike  2014-12-01\n",
       "1          10    Mike    Address for Mike  2014-12-01\n",
       "2          11  Marcia  Address for Marcia  2014-12-01"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers.merge(orders)     # merge customers and orders so we can ship the items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c733a50",
   "metadata": {},
   "source": [
    "pandas has done something magical for us here by being able to accomplish this with such a simple piece of code. What pandas has done is realized that our customers and orders objects both have a column named CustomerID. With this knowledge, it uses common values found in that column of both DataFrame objects to relate the data in both and form the merged data based on inner join semantics. To be even more detailed, what pandas has specifically done is the following:\n",
    "\n",
    "1. Determines the columns in both customers and orders with common labels. These columns are treated as the keys to perform the join.\n",
    "\n",
    "2. It creates a new DataFrame whose columns are the labels from the keys identified in step 1, followed by all of the non-key labels from both objects. \n",
    "\n",
    "3. It matches values in the key columns of both DataFrame objects.\n",
    "\n",
    "4. It then creates a row in the result for each set of matching labels.\n",
    "\n",
    "5. It then copies the data from those matching rows from each source object into that respective row and columns of the result.\n",
    "\n",
    "6. It assigns a new Int64Index to the result.\n",
    "\n",
    "The join in a merge can use values from multiple columns. To demonstrate, the following creates two DataFrame objects and performs the merge where pandas decides to use the values in the key1 and key2 columns of both objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4e2f83b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1\n",
       "0    a    x      0\n",
       "1    b    y      1\n",
       "2    c    z      2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to be used in the remainder of this section's examples\n",
    "\n",
    "left_data = {'key1': ['a', 'b', 'c'],\n",
    "             'key2': ['x', 'y', 'z'],\n",
    "             'lval1': [ 0, 1, 2]}\n",
    "\n",
    "right_data = {'key1': ['a', 'b', 'c'],\n",
    "              'key2': ['x', 'a', 'z'],\n",
    "              'rval1': [ 6, 7, 8 ]}\n",
    "\n",
    "left = pd.DataFrame(left_data, index=[0, 1, 2])\n",
    "right = pd.DataFrame(right_data, index=[1, 2, 3])\n",
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "759b80e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  rval1\n",
       "1    a    x      6\n",
       "2    b    a      7\n",
       "3    c    z      8"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c23ac3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1  rval1\n",
       "0    a    x      0      6\n",
       "1    c    z      2      8"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.merge(right)     # demonstrate merge without specifying columns to merg this will implicitly merge on all common columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7bfbd6",
   "metadata": {},
   "source": [
    "This merge identifies key1 and key2 columns as common in both DataFrame objects and hence uses them for the merge. The matching tuples of values in both DataFrame objects for these columns are (a, x) and (c, z) and therefore this results\n",
    "in two rows of values.\n",
    "\n",
    "To explicitly specify which column use to relate the objects, use the on parameter. The following performs a merge using only the values in the key1 column of both DataFrame objects:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1bf16164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2_x  lval1 key2_y  rval1\n",
       "0    a      x      0      x      6\n",
       "1    b      y      1      a      7\n",
       "2    c      z      2      z      8"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.merge(right, on='key1')# demonstrate merge using an explicit column on needs the value to be in both DataFrame objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d503522",
   "metadata": {},
   "source": [
    "Comparing this result to the previous example, as only the values in the key1 column were used to relate the data in the two objects, the result now has three rows as there are matching a, b, and c values in that single column of both objects.\n",
    "\n",
    "The on parameter can also be given a list of column names. The following reverts to using both the key1 and key2 columns, resulting in being identical the earlier example where those two columns where implicitly identified by pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d908ea31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1  rval1\n",
       "0    a    x      0      6\n",
       "1    c    z      2      8"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.merge(right, on=['key1', 'key2'])# merge explicitly using two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd2ffa",
   "metadata": {},
   "source": [
    "The columns specified with on need to exist in both DataFrame objects. If you would like to merge based on columns with different names in each object, you can use the left_on and right_on parameters, passing the name or names of columns to each\n",
    "respective parameter.\n",
    "\n",
    "To perform a merge with the labels of the row indexes of the two DataFrame objects, use\n",
    "the left_index=True and right_index=True parameters (both need to be specified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3322697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1_x key2_x  lval1 key1_y key2_y  rval1\n",
       "1      b      y      1      a      x      6\n",
       "2      c      z      2      b      a      7"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(left, right, left_index=True, right_index=True) # join on the row indices of both matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577bd85",
   "metadata": {},
   "source": [
    "This has identified that the index labels in common are 1 and 2, so the resulting DataFrame has two rows with these values and labels in the index. pandas then creates a column in the result for every column in both objects and then copies the values.\n",
    "\n",
    "As both DataFrame objects had a column with an identical name, key, the columns in the result have the _x and _y suffixes appended to them to identify the DataFrame they originated from. _x is for left and _y for right. You can specify these suffixes using the suffixes parameter and passing a two-item sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e3b6d",
   "metadata": {},
   "source": [
    "### <div class= \"alert alert-warning\">Specifying the join semantics of a merge operation\n",
    "    \n",
    "<mark>The default type of join performed by pd.merge() is an inner join</mark>. To use another join method, the method of join to be used can be specified using the below parameter of the pd.merge() function (or the .merge() method). The valid options are:\n",
    "    \n",
    "• <b>inner:</b> This is the intersection of keys from both DataFrame objects\n",
    "    \n",
    "• <b>outer:</b> This is the union of keys from both DataFrame objects\n",
    "    \n",
    "• <b>left: </b>This only uses keys from the left DataFrame \n",
    "    \n",
    "• <b>right:</b> This only uses keys from the right DataFrame\n",
    "    \n",
    "As we have seen, an inner join is the default and will return a merge of the data from both DataFrame objects only where the values match. \n",
    "    \n",
    "An outer join contrasts, in that it will return both the merge of the matched rows and the unmatched values from both the left and right DataFrame objects, but with NaN \n",
    "    filled in the unmatched portion. The following code demonstrates an outer join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27180ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1  rval1\n",
       "0    a    x    0.0    6.0\n",
       "1    b    y    1.0    NaN\n",
       "2    c    z    2.0    8.0\n",
       "3    b    a    NaN    7.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.merge(right, how='outer') # outer join, merges all matched data, and fills unmatched items with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb507be",
   "metadata": {},
   "source": [
    "A left join will return the merge of the rows that satisfy the join of the values in the specified columns, and also returns the unmatched rows from only left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e786a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1  rval1\n",
       "0    a    x      0    6.0\n",
       "1    b    y      1    NaN\n",
       "2    c    z      2    8.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# left join, merges all matched data, and only fills unmatched\n",
    "# items from the left dataframe with NaN filled for the\n",
    "# unmatched items in the result\n",
    "# rows with labels 0 and 2\n",
    "# match on key1 and key2 the row with label 1 is from left\n",
    "left.merge(right, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8950cce",
   "metadata": {},
   "source": [
    "A right join will return the merge of the rows that satisfy the join of the values in the specified columns, and also returns the unmatched rows from only right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a2cb8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1 key2  lval1  rval1\n",
       "0    a    x    0.0      6\n",
       "1    b    a    NaN      7\n",
       "2    c    z    2.0      8"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# right join, merges all matched data, and only fills unmatched\n",
    "# item from the right with NaN filled for the unmatched items\n",
    "# in the result\n",
    "# rows with labels 0 and 1 match on key1 and key2\n",
    "# the row with label 2 is from right\n",
    "left.merge(right, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816b5dd",
   "metadata": {},
   "source": [
    "The pandas library also provides a <span style=\"color:red\">.join()</span> method that can be used to perform a join using the index labels of the two DataFrame objects (instead of values in columns).<mark> Note that if the columns in the two DataFrame objects do not have unique column names, you must specify suffixes using the lsuffix and rsuffix parameters (automatic suffixing is not performed). The following code demonstrates both the join and specification of suffixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c288b140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1_left key2_left  lval1 key1_right key2_right  rval1\n",
       "0         a         x      0        NaN        NaN    NaN\n",
       "1         b         y      1          a          x    6.0\n",
       "2         c         z      2          b          a    7.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join left with right (default method is outer)\n",
    "# and since these DataFrame objects have duplicate column names\n",
    "# we just specify lsuffix and rsuffix\n",
    "left.join(right, lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bda146c",
   "metadata": {},
   "source": [
    "The default type of join performed is an outer join. Note that this differs from the default of the .merge() method, which defaults to inner. To change to an inner join, specify how='inner', as is demonstrated in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42757c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  key1_left key2_left  lval1 key1_right key2_right  rval1\n",
       "1         b         y      1          a          x      6\n",
       "2         c         z      2          b          a      7"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.join(right, lsuffix='_left', rsuffix='_right', how='inner')  # join left with right with an inner join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcefcf55",
   "metadata": {},
   "source": [
    " Notice that this is roughly equivalent to the earlier result from In[92] except with the result having columns with slightly different names.\n",
    " \n",
    "It is also possible to perform right and left joins, but they lead to results similar to previous examples, so they will be omitted for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d24cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" >\n",
    "    \n",
    "# <div class=\"alert alert-success\" > Pivoting\n",
    "    \n",
    " Data is often stored in a stacked format, which is also referred to as record format; this is common in databases, .csv files, and Excel spreadsheets. In a stacked format, the data is often not normalized and has repeated values in many columns, or values that should logically exists in other tables (violating another concept of tidy data). Take the following data, which represents a stream of data from an accelerometer on a mobile device (provided with the data from the sample code):   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5c78f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    interval axis  reading\n",
       "0          0    x      0.0\n",
       "1          0    y      0.5\n",
       "2          0    z      1.0\n",
       "3          1    x      0.1\n",
       "4          1    y      0.4\n",
       "..       ...  ...      ...\n",
       "7          2    y      0.3\n",
       "8          2    z      0.8\n",
       "9          3    x      0.3\n",
       "10         3    y      0.2\n",
       "11         3    z      0.7\n",
       "\n",
       "[12 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"Datasets/accel.csv\"\n",
    "sensor_readings = pd.read_csv(path)\n",
    "sensor_readings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ac39f",
   "metadata": {},
   "source": [
    "An issue with this data as it is organized is: how does one go about determining the\n",
    "readings for a specific axis? This can be naively done with Boolean selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79a8a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   interval axis  reading\n",
       "0         0    x      0.0\n",
       "3         1    x      0.1\n",
       "6         2    x      0.2\n",
       "9         3    x      0.3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract X-axis readings\n",
    "sensor_readings[sensor_readings['axis'] == 'x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ebe9a4",
   "metadata": {},
   "source": [
    "An issue here is what if you want to know the values for all axes at a given time, not just the x axis? You can perform a selection for each value of the axis, but that is repetitive code and does not handle the scenario of new axis values being inserted into DataFrame without a change to the code. \n",
    "\n",
    "A better representation would be where columns represent the unique variable values. To convert to this form, use the DataFrame objects' <span style=\"color:red\">.pivot()</span> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c6ae66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "axis        x    y    z\n",
       "interval               \n",
       "0         0.0  0.5  1.0\n",
       "1         0.1  0.4  0.9\n",
       "2         0.2  0.3  0.8\n",
       "3         0.3  0.2  0.7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot the data. Interval becomes the index, the columns are the current axes values, and use the readings as values\n",
    "\n",
    "sensor_readings.pivot(index='interval',columns='axis',values='reading')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7f2b0",
   "metadata": {},
   "source": [
    "This has taken all of the distinct values from the axis column, and pivoted them into columns on the new DataFrame, while filling in values for the new columns from the appropriate rows and columns of the original DataFrame. This new DataFrame demonstrates that it is now very easy to identify the X, Y and Z sensor readings at each time interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709c8d36",
   "metadata": {},
   "source": [
    "# <div class=\"alert alert-danger\" > Extra\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3f58173",
   "metadata": {},
   "source": [
    "### Hierarchical indexes:\n",
    "\n",
    "<b>1.</b> Hierarchical indexing isalso known as multi-indexing. Hierarchical indexing is a method of creating structured group relationships in data. These hierarchical indexes, or MultiIndexes, are highly flexible and offer a range of options when performing complex data queries(masks/filters).\n",
    "\n",
    "<b>2.</b> The index is like an address, that’s how any data point across the data frame or series can be accessed. Rows and columns both have indexes, rows indices are called index and for columns, it’s general column names.Hierarchical Indexes are also known as multi-indexing is setting more than one column name as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d45f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0              region       state  individuals  family_members  \\\n",
       "0           0  East South Central     Alabama       2570.0           864.0   \n",
       "1           1             Pacific      Alaska       1434.0           582.0   \n",
       "2           2            Mountain     Arizona       7259.0          2606.0   \n",
       "3           3  West South Central    Arkansas       2280.0           432.0   \n",
       "4           4             Pacific  California     109008.0         20964.0   \n",
       "\n",
       "   state_pop  \n",
       "0    4887681  \n",
       "1     735139  \n",
       "2    7158024  \n",
       "3    3009733  \n",
       "4   39461588  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Datasets/homelessness.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b115fe4f",
   "metadata": {},
   "source": [
    "In the following data frame, there is no indexing.\n",
    "\n",
    "#### Columns in the Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26db9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'region', 'state', 'individuals', 'family_members',\n",
       "       'state_pop'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = df.columns\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4b05a",
   "metadata": {},
   "source": [
    "<mark>To make the column an index, we use the Set_index() function of pandas. If we want to make one column an index, we can simply pass the name of the column as a string in set_index(). If we want to do multi-indexing or Hierarchical Indexing, we pass the list of column names in the set_index().</mark>\n",
    "\n",
    "#### Below Code demonstrates Hierarchical Indexing in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce0ffba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                     Unnamed: 0  \\\n",
       "region             state                individuals               \n",
       "East South Central Alabama              2570.0                0   \n",
       "Pacific            Alaska               1434.0                1   \n",
       "Mountain           Arizona              7259.0                2   \n",
       "West South Central Arkansas             2280.0                3   \n",
       "Pacific            California           109008.0              4   \n",
       "Mountain           Colorado             7607.0                5   \n",
       "New England        Connecticut          2280.0                6   \n",
       "South Atlantic     Delaware             708.0                 7   \n",
       "                   District of Columbia 3770.0                8   \n",
       "                   Florida              21443.0               9   \n",
       "\n",
       "                                                     family_members  state_pop  \n",
       "region             state                individuals                             \n",
       "East South Central Alabama              2570.0                864.0    4887681  \n",
       "Pacific            Alaska               1434.0                582.0     735139  \n",
       "Mountain           Arizona              7259.0               2606.0    7158024  \n",
       "West South Central Arkansas             2280.0                432.0    3009733  \n",
       "Pacific            California           109008.0            20964.0   39461588  \n",
       "Mountain           Colorado             7607.0               3250.0    5691287  \n",
       "New England        Connecticut          2280.0               1696.0    3571520  \n",
       "South Atlantic     Delaware             708.0                 374.0     965479  \n",
       "                   District of Columbia 3770.0               3134.0     701547  \n",
       "                   Florida              21443.0              9587.0   21244317  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the pandas set_index() function.\n",
    "df_ind3 = df.set_index(['region', 'state', 'individuals'])\n",
    "  \n",
    "# we can sort the data by using sort_index()\n",
    "df_ind3.sort_index()\n",
    "  \n",
    "df_ind3.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408e06f9",
   "metadata": {},
   "source": [
    "Now the dataframe is using Hierarchical Indexing or multi-indexing.\n",
    "\n",
    "Note that here we have made 3 columns as an index (‘region’, ‘state’, ‘individuals’ ). The first index ‘region’ is called level(0) index, which is on top of the Hierarchy of indexes, next index ‘state’ is level(1) index which is below the main or level(0) index, and so on. So, the Hierarchy of indexes is formed that’s why this is called Hierarchical indexing.\n",
    "\n",
    "We may sometimes need to make a column as an index, or we want to convert an index column into the normal column, so there is a pandas reset_index(inplace = True) function, which makes the index column the normal column.\n",
    "\n",
    "#### Selecting Data in a Hierarchical Index or using the Hierarchical Indexing:\n",
    "\n",
    "For selecting the data from the dataframe using the .loc() method we have to pass the name of the indexes in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b20840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Unnamed: 0  family_members  state_pop\n",
      "region   state      individuals                                       \n",
      "Pacific  Alaska     1434.0                1           582.0     735139\n",
      "         California 109008.0              4         20964.0   39461588\n",
      "         Hawaii     4131.0               11          2399.0    1420593\n",
      "         Oregon     11139.0              37          3337.0    4181886\n",
      "         Washington 16424.0              47          5880.0    7523869\n",
      "Mountain Arizona    7259.0                2          2606.0    7158024\n",
      "         Colorado   7607.0                5          3250.0    5691287\n",
      "         Idaho      1297.0               12           715.0    1750536\n",
      "         Montana    983.0                26           422.0    1060665\n",
      "         Nevada     7058.0               28           486.0    3027341\n"
     ]
    }
   ],
   "source": [
    "# selecting the 'Pacific' and 'Mountain' \n",
    "# region from the dataframe.\n",
    "  \n",
    "# selecting data using level(0) index or main index.\n",
    "df_ind3_region = df_ind3.loc[['Pacific', 'Mountain']]\n",
    "  \n",
    "print(df_ind3_region.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
